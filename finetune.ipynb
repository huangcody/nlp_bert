{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYtVvEwzngbn"
   },
   "outputs": [],
   "source": "# 套件安裝指令\n!pip install --extra-index-url https://download.pytorch.org/whl/cu125 torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 accelerate==1.8.1 transformers datasets evaluate scikit-learn google.colab pandas"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 掛載 Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn9TXerTole6"
   },
   "outputs": [],
   "source": [
    "# 會需要使用者同意授權/存取 Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R04dIS9QqD0o"
   },
   "outputs": [],
   "source": [
    "# 切換目錄 (Colab 預設目錄為 /content，使用 %cd 切換目錄)\n",
    "%cd /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 觀看系統設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGSOsvEIpswK"
   },
   "outputs": [],
   "source": [
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SI-_WjfwUA1"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7V-wNylwglC"
   },
   "outputs": [],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微調模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROCfoeHI9ak0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匯入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n)\n\nimport random\nimport json\nimport pandas as pd\nfrom sklearn.metrics import f1_score"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本參數設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_name = 'google-bert/bert-base-chinese' # 預訓練模型名稱\nmax_seq_length = 512 # 可訓練的序列最大長度\nnum_labels = 8 # 多元分類 (8 種情緒)\noutput_dir = './output' # 輸出模型資料夾\n\n# 載入 tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 讀取 HuggingFace 資料集"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 設定隨機種子，確保實驗可重現\nseed = 42\n\n# 從 HuggingFace 讀取資料集\ndf = pd.read_parquet(\"hf://datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset/data/train-00000-of-00001.parquet\")\n\n# 查看資料集基本資訊\nprint(f\"資料集大小: {len(df)}\")\nprint(f\"欄位: {df.columns.tolist()}\")\nprint(f\"情緒類別: {df['emotion'].unique().tolist()}\")\nprint(f\"各類別數量:\\n{df['emotion'].value_counts()}\")\n\n# 建立 label 對應表 (文字 -> 整數)\nlabel_list = sorted(df['emotion'].unique().tolist())\nlabel2id = {label: idx for idx, label in enumerate(label_list)}\nid2label = {idx: label for label, idx in label2id.items()}\n\nprint(f\"\\nLabel 對應表:\")\nfor label, idx in label2id.items():\n    print(f\"  {label} -> {idx}\")\n\n# 將文字 label 轉換為整數\ndf['labels'] = df['emotion'].map(label2id)\n\n# 洗牌\ndf = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\nsentences = df['text'].tolist()\nlabels = df['labels'].tolist()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 轉換成 huggingface trainer 可以使用的 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'sentences': sentences,\n",
    "    'labels': labels\n",
    "})\n",
    "\n",
    "# 回傳切分資料 (訓練 和 驗證)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "'''\n",
    "print(dataset) 的內容如下:\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['sentences', 'labels'],\n",
    "        num_rows: 6212\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['sentences', 'labels'],\n",
    "        num_rows: 1554\n",
    "    })\n",
    "})\n",
    "'''\n",
    "\n",
    "# 預處理資料\n",
    "def preprocess_data(dataset):\n",
    "    # 將句子轉換為 token (tokenization)\n",
    "    return tokenizer(\n",
    "        dataset['sentences'], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        return_tensors='pt', \n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "\n",
    "# 轉換資料\n",
    "train_data = dataset['train'].map(preprocess_data, batched=True)\n",
    "valid_data = dataset['test'].map(preprocess_data, batched=True)\n",
    "\n",
    "# 建立 DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'test': valid_data\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定模型評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEVAYBkXxzkH"
   },
   "outputs": [],
   "source": "# 計算模型評估指標\ndef compute_metrics(predicted_results):\n    # 取得真實標籤\n    labels = predicted_results.label_ids\n\n    # 取得預測結果\n    preds = predicted_results.predictions.argmax(-1)\n\n    # 計算 F1 score (多元分類使用 weighted)\n    # 可參考: https://blog.csdn.net/qq_40671063/article/details/130447922\n    f1 = f1_score(labels, preds, average='weighted')\n    return {\n        'f1': f1,\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微調模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3epMxp60-Y4r"
   },
   "outputs": [],
   "source": "# 讀取模型 (設定 id2label 和 label2id，讓模型知道每個 id 對應的標籤名稱)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n)\n\n# 設定訓練參數\ntraining_args = TrainingArguments(\n    output_dir=output_dir, # 輸出資料夾\n    overwrite_output_dir=True,\n    num_train_epochs=3, # 訓練回合數\n    per_device_train_batch_size=32, # 批次大小\n    per_device_eval_batch_size=32, # 批次大小\n    gradient_accumulation_steps=2, # 梯度累積步數，主要是為了讓較小的 GPU 也能訓練較大的 batch size\n    learning_rate=0.00005 * 2, # 預設會將每個小 batch 的 loss 除以累積步數（loss / 2），讓最終梯度等同於單一大 batch，但這會讓梯度變小（等於將 learning rate 除以 2），所以要手動將 learning rate 乘以累積步數來抵消\n    warmup_steps=50, # 預熱步數，主要是讓 learning rate 從 0 緩慢增加到設定的 learning rate，避免一開始學習率過大導致模型不穩定\n    weight_decay=0.01, # 權重衰減，主要是為了正則化，避免過擬合，讓 loss 曲線較平滑\n    eval_strategy=\"steps\", # epoch, steps, no\n    eval_steps=70, # 每隔多少步驟評估一次\n    save_strategy=\"steps\", # epoch, steps, no\n    save_steps=70, # 每隔多少步驟儲存一次\n    save_total_limit=2, # 最多儲存模型數量\n    load_best_model_at_end=True, # Trainer 會追蹤 metric_for_best_model（預設 eval_loss，越小越好）來判斷最佳模型，訓練結束時自動將該最佳檢查點載入記憶體覆蓋最終模型\n    seed=seed, # 隨機種子，主要是為了確保實驗可重現\n    lr_scheduler_type=\"linear\", # https://blog.csdn.net/muyao987/article/details/139319466\n    report_to=\"none\", # 關閉內建的實驗追蹤功能 (如 wandb, tensorboard)\n)\n\n# 設定 Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset_dict['train'],\n    eval_dataset=dataset_dict['test'],\n    data_collator=None, # DataCollatorWithPadding(tokenizer),\n    compute_metrics=compute_metrics,\n)\n\n# 開始訓練\ntrainer.train()\n\n# 儲存模型\ntrainer.save_model(output_dir)\n\n# 儲存 tokenizer\ntokenizer.save_pretrained(output_dir)\n\n# 儲存 label 對應表 (方便預測時使用)\nimport json\nwith open(f'{output_dir}/label_mapping.json', 'w', encoding='utf-8') as f:\n    json.dump({'label2id': label2id, 'id2label': {str(k): v for k, v in id2label.items()}}, f, ensure_ascii=False, indent=2)\n\nprint(f\"模型已儲存至 {output_dir}\")\nprint(f\"Label 對應表已儲存至 {output_dir}/label_mapping.json\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMKWLyaNDZnqlM6q3MHE3gZ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}